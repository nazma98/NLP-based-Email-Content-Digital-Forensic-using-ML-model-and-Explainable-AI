{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics extracted from LDA model\n",
    "\n",
    "my_topics= [(0,\n",
    "  '0.027*\"gas\" + 0.022*\"deal\" + 0.013*\"price\" + 0.008*\"change\" + 0.007*\"day\" + '\n",
    "  '0.007*\"volume\" + 0.007*\"contract\" + 0.006*\"need\" + 0.006*\"product\" + '\n",
    "  '0.006*\"pipeline\"'),\n",
    " (1,\n",
    "  '0.039*\"final\" + 0.029*\"schedule\" + 0.026*\"hour\" + 0.022*\"variance\" + '\n",
    "  '0.018*\"date\" + 0.014*\"scid\" + 0.014*\"transdate\" + 0.014*\"transtype\" + '\n",
    "  '0.014*\"mkttype\" + 0.013*\"detect\"'),\n",
    " (2,\n",
    "  '0.020*\"game\" + 0.015*\"texas\" + 0.012*\"play\" + 0.011*\"team\" + 0.009*\"update\" '\n",
    "  '+ 0.008*\"week\" + 0.008*\"season\" + 0.007*\"ut\" + 0.007*\"brown\" + '\n",
    "  '0.007*\"player\"'),\n",
    " (3,\n",
    "  '0.018*\"enron\" + 0.016*\"agreement\" + 0.011*\"send\" + 0.010*\"pm\" + '\n",
    "  '0.010*\"attach\" + 0.009*\"intend\" + 0.008*\"fax\" + 0.008*\"copy\" + 0.007*\"corp\" '\n",
    "  '+ 0.007*\"may\"'),\n",
    " (4,\n",
    "  '0.012*\"information\" + 0.011*\"click\" + 0.009*\"service\" + 0.008*\"new\" + '\n",
    "  '0.008*\"use\" + 0.008*\"receive\" + 0.008*\"access\" + 0.007*\"online\" + '\n",
    "  '0.007*\"http\" + 0.007*\"report\"'),\n",
    " (5,\n",
    "  '0.019*\"get\" + 0.014*\"go\" + 0.011*\"would\" + 0.011*\"know\" + 0.009*\"think\" + '\n",
    "  '0.009*\"good\" + 0.009*\"like\" + 0.008*\"one\" + 0.008*\"time\" + 0.008*\"want\"'),\n",
    " (6,\n",
    "  '0.023*\"enron\" + 0.022*\"company\" + 0.011*\"say\" + 0.009*\"million\" + '\n",
    "  '0.007*\"new\" + 0.007*\"market\" + 0.006*\"stock\" + 0.006*\"business\" + '\n",
    "  '0.006*\"year\" + 0.005*\"financial\"'),\n",
    " (7,\n",
    "  '0.092*\"td\" + 0.087*\"image\" + 0.063*\"font\" + 0.052*\"tr\" + 0.049*\"br\" + '\n",
    "  '0.021*\"table\" + 0.018*\"width\" + 0.014*\"color\" + 0.014*\"size\" + '\n",
    "  '0.013*\"border\"'),\n",
    " (8,\n",
    "  '0.012*\"travel\" + 0.010*\"ticket\" + 0.009*\"houston\" + 0.007*\"special\" + '\n",
    "  '0.007*\"new\" + 0.007*\"fare\" + 0.007*\"city\" + 0.006*\"hotel\" + 0.006*\"tx\" + '\n",
    "  '0.006*\"tofrom\"'),\n",
    " (9,\n",
    "  '0.141*\"ect\" + 0.095*\"enron\" + 0.024*\"pm\" + 0.021*\"ees\" + 0.017*\"enronxgate\" '\n",
    "  '+ 0.011*\"senron\" + 0.008*\"mark\" + 0.008*\"david\" + 0.007*\"john\" + '\n",
    "  '0.006*\"communications\"'),\n",
    " (10,\n",
    "  '0.024*\"enron\" + 0.009*\"business\" + 0.009*\"group\" + 0.008*\"meeting\" + '\n",
    "  '0.007*\"vince\" + 0.007*\"work\" + 0.007*\"would\" + 0.006*\"management\" + '\n",
    "  '0.006*\"houston\" + 0.006*\"team\"'),\n",
    " (11,\n",
    "  '0.038*\"pm\" + 0.034*\"outage\" + 0.026*\"database\" + 0.026*\"schedule\" + '\n",
    "  '0.026*\"thru\" + 0.021*\"sit\" + 0.020*\"error\" + 0.017*\"impact\" + 0.017*\"ct\" + '\n",
    "  '0.017*\"london\"'),\n",
    " (12,\n",
    "  '0.037*\"send\" + 0.034*\"original\" + 0.027*\"pm\" + 0.011*\"john\" + '\n",
    "  '0.010*\"october\" + 0.010*\"monday\" + 0.009*\"wednesday\" + 0.009*\"fw\" + '\n",
    "  '0.009*\"tuesday\" + 0.009*\"thursday\"'),\n",
    " (13,\n",
    "  '0.026*\"power\" + 0.019*\"energy\" + 0.017*\"california\" + 0.016*\"say\" + '\n",
    "  '0.014*\"state\" + 0.011*\"price\" + 0.010*\"electricity\" + 0.009*\"utility\" + '\n",
    "  '0.007*\"plant\" + 0.006*\"market\"'),\n",
    " (14,\n",
    "  '0.011*\"would\" + 0.008*\"issue\" + 0.007*\"market\" + 0.005*\"contract\" + '\n",
    "  '0.005*\"may\" + 0.005*\"provide\" + 0.005*\"customer\" + 0.005*\"need\" + '\n",
    "  '0.005*\"make\" + 0.005*\"time\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9aacb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.027*\"gas\" + 0.022*\"deal\" + 0.013*\"price\" + 0.008*\"change\" + 0.007*\"day\" + 0.007*\"volume\" + 0.007*\"contract\" + 0.006*\"need\" + 0.006*\"product\" + 0.006*\"pipeline\"')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91207e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"gas\"', '\"deal\"', '\"price\"', '\"change\"', '\"day\"', '\"volume\"', '\"contract\"', '\"need\"', '\"product\"', '\"pipeline'], ['\"final\"', '\"schedule\"', '\"hour\"', '\"variance\"', '\"date\"', '\"scid\"', '\"transdate\"', '\"transtype\"', '\"mkttype\"', '\"detect'], ['\"game\"', '\"texas\"', '\"play\"', '\"team\"', '\"update\"', '\"week\"', '\"season\"', '\"ut\"', '\"brown\"', '\"player'], ['\"enron\"', '\"agreement\"', '\"send\"', '\"pm\"', '\"attach\"', '\"intend\"', '\"fax\"', '\"copy\"', '\"corp\"', '\"may'], ['\"information\"', '\"click\"', '\"service\"', '\"new\"', '\"use\"', '\"receive\"', '\"access\"', '\"online\"', '\"http\"', '\"report'], ['\"get\"', '\"go\"', '\"would\"', '\"know\"', '\"think\"', '\"good\"', '\"like\"', '\"one\"', '\"time\"', '\"want'], ['\"enron\"', '\"company\"', '\"say\"', '\"million\"', '\"new\"', '\"market\"', '\"stock\"', '\"business\"', '\"year\"', '\"financial'], ['\"td\"', '\"image\"', '\"font\"', '\"tr\"', '\"br\"', '\"table\"', '\"width\"', '\"color\"', '\"size\"', '\"border'], ['\"travel\"', '\"ticket\"', '\"houston\"', '\"special\"', '\"new\"', '\"fare\"', '\"city\"', '\"hotel\"', '\"tx\"', '\"tofrom'], ['\"ect\"', '\"enron\"', '\"pm\"', '\"ees\"', '\"enronxgate\"', '\"senron\"', '\"mark\"', '\"david\"', '\"john\"', '\"communications'], ['\"enron\"', '\"business\"', '\"group\"', '\"meeting\"', '\"vince\"', '\"work\"', '\"would\"', '\"management\"', '\"houston\"', '\"team'], ['\"pm\"', '\"outage\"', '\"database\"', '\"schedule\"', '\"thru\"', '\"sit\"', '\"error\"', '\"impact\"', '\"ct\"', '\"london'], ['\"send\"', '\"original\"', '\"pm\"', '\"john\"', '\"october\"', '\"monday\"', '\"wednesday\"', '\"fw\"', '\"tuesday\"', '\"thursday'], ['\"power\"', '\"energy\"', '\"california\"', '\"say\"', '\"state\"', '\"price\"', '\"electricity\"', '\"utility\"', '\"plant\"', '\"market'], ['\"would\"', '\"issue\"', '\"market\"', '\"contract\"', '\"may\"', '\"provide\"', '\"customer\"', '\"need\"', '\"make\"', '\"time']]\n"
     ]
    }
   ],
   "source": [
    "topic_keywords = []\n",
    "for topic_info in my_topics:\n",
    "  # Extract keywords string\n",
    "  keywords_str = topic_info[1].strip()[1:-1]  # Remove quotes and leading/trailing spaces\n",
    "\n",
    "  # Split keywords (keep everything after the asterisk)\n",
    "  keywords = [word.split(\"*\", 1)[1].strip() for word in keywords_str.split(\"+\")]\n",
    "  topic_keywords.append(keywords)\n",
    "\n",
    "# Now you have a list of lists, where each inner list represents the keywords for a topic\n",
    "print(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb01663",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9547/1254493776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Create word cloud object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# Plot the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[1;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[1;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# find font sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    509\u001b[0m                     font, orientation=orientation)\n\u001b[1;32m    510\u001b[0m                 \u001b[0;31m# get size of resulting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0mbox_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0;31m# find possible places using integral image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreeTypeFont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only supported for TrueType fonts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         bbox = font.getbbox(\n",
      "\u001b[0;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have topic_keywords (list of lists, each inner list containing topic keywords)\n",
    "\n",
    "# Set stopwords (optional)\n",
    "stopwords = set(STOPWORDS)  # Add custom stopwords if needed\n",
    "\n",
    "# Create and plot word clouds for each topic\n",
    "for i, topic_words in enumerate(topic_keywords):\n",
    "  # Filter out stopwords\n",
    "  filtered_words = [word for word in topic_words if word not in stopwords]\n",
    "  # Create word cloud object\n",
    "  wordcloud = WordCloud(width=800, height=600, stopwords=stopwords).generate(\" \".join(filtered_words))\n",
    "\n",
    "  # Plot the word cloud\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.imshow(wordcloud, interpolation='nearest')\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(f\"Topic {i + 1}\")\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e98c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anytime fine fletch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>tana joneshouect molly harris pm frank davisho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>update say delay since spare moment look anywa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>hate honestly know handle attach think one mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>r june june ecs ecn network interconnection ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413915</th>\n",
       "      <td>413915</td>\n",
       "      <td>413915</td>\n",
       "      <td>weekend original mailto send monday february o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413916</th>\n",
       "      <td>413916</td>\n",
       "      <td>413916</td>\n",
       "      <td>interest attend original bean kenneth send wed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413917</th>\n",
       "      <td>413917</td>\n",
       "      <td>413917</td>\n",
       "      <td>give judy townsend scott goodell ability trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413918</th>\n",
       "      <td>413918</td>\n",
       "      <td>413918</td>\n",
       "      <td>greeting everyone want come housewarme party m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413919</th>\n",
       "      <td>413919</td>\n",
       "      <td>413919</td>\n",
       "      <td>inline attachment follow trosper mary trosper ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413920 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  \\\n",
       "0                  0           0   \n",
       "1                  1           1   \n",
       "2                  2           2   \n",
       "3                  3           3   \n",
       "4                  4           4   \n",
       "...              ...         ...   \n",
       "413915        413915      413915   \n",
       "413916        413916      413916   \n",
       "413917        413917      413917   \n",
       "413918        413918      413918   \n",
       "413919        413919      413919   \n",
       "\n",
       "                                                  content  \n",
       "0                                     anytime fine fletch  \n",
       "1       tana joneshouect molly harris pm frank davisho...  \n",
       "2       update say delay since spare moment look anywa...  \n",
       "3       hate honestly know handle attach think one mig...  \n",
       "4       r june june ecs ecn network interconnection ju...  \n",
       "...                                                   ...  \n",
       "413915  weekend original mailto send monday february o...  \n",
       "413916  interest attend original bean kenneth send wed...  \n",
       "413917  give judy townsend scott goodell ability trade...  \n",
       "413918  greeting everyone want come housewarme party m...  \n",
       "413919  inline attachment follow trosper mary trosper ...  \n",
       "\n",
       "[413920 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "document = pd.read_csv('/home/nazma/NLP-based-Email-Content-Digital-Forensic-using-ML-model-and-Explainable-AI/email-content-preprocessing/basic_lemmatized_content.csv')\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5d5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [53:02,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1441it [3:48:23,  4.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2439it [5:46:13,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3436it [7:52:06,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4432it [10:08:49, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5427it [12:29:13, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6424it [15:02:59, 42.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7417it [17:07:37,  5.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8412it [19:34:39,  4.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9409it [22:11:23, 12.97s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10403it [24:41:31,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 139539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10406it [24:41:49,  8.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9547/1551909298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Extract features chunk-wise and save to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mextract_topic_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_keywords\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use copy to avoid modifying original DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic features saved to topic_features_119600.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9547/1551909298.py\u001b[0m in \u001b[0;36mextract_topic_features\u001b[0;34m(document, topic_keywords)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Check for any keyword in the current topic (all words in topic_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtopic_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                   \u001b[0;32mif\u001b[0m \u001b[0msearch_topic_word_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check for a match with any word in topic_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9547/1551909298.py\u001b[0m in \u001b[0;36msearch_topic_word_regex\u001b[0;34m(document, topic_word)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Example pattern matching singular and plural forms (lemmatized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minflect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create inflect engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mrf\"\\b{topic_word}\\b|\\b{p.plural_noun(topic_word)}\\b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/inflect/__init__.py\u001b[0m in \u001b[0;36mplural_noun\u001b[0;34m(self, text, count)\u001b[0m\n\u001b[1;32m   2406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2408\u001b[0;31m         \u001b[0mplural\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plnoun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{pre}{plural}{post}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/inflect/__init__.py\u001b[0m in \u001b[0;36m_plnoun\u001b[0;34m(self, word, count)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpl_sb_uninflected_complete\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_long_compounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import inflect\n",
    "from tqdm import tqdm  # Import tqdm for progress bar (optional)\n",
    "\n",
    "\n",
    "def search_topic_word_regex(document, topic_word):\n",
    "  \"\"\"\n",
    "  Searches for a specific topic word (and potential variations) in a lemmatized document (using regular expressions).\n",
    "\n",
    "  Args:\n",
    "      document (str): The lemmatized text of a document.\n",
    "      topic_word (str): The topic word to search for.\n",
    "\n",
    "  Returns:\n",
    "      bool: True if the topic word (or a variation) is found in the document, False otherwise.\n",
    "  \"\"\"\n",
    "  # Example pattern matching singular and plural forms (lemmatized)\n",
    "  p = inflect.engine()  # Create inflect engine\n",
    "  pattern = rf\"\\b{topic_word}\\b|\\b{p.plural_noun(topic_word)}\\b\"\n",
    "  return bool(re.search(pattern, document))\n",
    "\n",
    "\n",
    "def extract_topic_features(document, topic_keywords):\n",
    "  \"\"\"\n",
    "  Extracts topic features for documents in chunks and saves them to a CSV file.\n",
    "\n",
    "  Args:\n",
    "      document (pd.DataFrame): The DataFrame containing the document text in a column (e.g., \"content\").\n",
    "      topic_keywords (list): A list of lists, where each inner list contains \n",
    "                             the top keywords for a topic.\n",
    "      chunksize (int): The number of documents to process in each chunk (default: 1000).\n",
    "\n",
    "  Returns:\n",
    "      None: The function iterates through chunks and saves features to CSV.\n",
    "  \"\"\"\n",
    "  start_doc_id = 0  # Keep track of starting document ID for each chunk\n",
    "  topic_names = [f\"topic {i}\" for i in range(len(topic_keywords))]  # Create topic names\n",
    "\n",
    "  with open(\"topic_features_00.csv\", \"w\", newline=\"\") as csvfile:  # Open CSV in write mode to create header\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"doc_id\"] + topic_names)  # Write header with document ID and topic names\n",
    "\n",
    "  for doc_id, content in tqdm(document[\"content\"].iloc[start_doc_id:].items()):\n",
    "    # Use .iloc for integer-based indexing (assuming numeric index)\n",
    "\n",
    "    if isinstance(content, str):\n",
    "      # Process documents with string content\n",
    "      features = [0] * len(topic_keywords)\n",
    "      for i, topic_words in enumerate(topic_keywords):\n",
    "        for word in content.split():\n",
    "            # Check for any keyword in the current topic (all words in topic_words)\n",
    "            for topic_word in topic_words:\n",
    "                  if search_topic_word_regex(word, topic_word.strip('\"')):  # Check for a match with any word in topic_words\n",
    "                        features[i] += 1\n",
    "\n",
    "\n",
    "      # Save features to CSV (append mode)\n",
    "      with open(\"topic_features_00.csv\", \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([doc_id] + features)\n",
    "\n",
    "    if (doc_id - start_doc_id + 1) % 1000 == 0:\n",
    "#       start_doc_id = doc_id + 1  # Update starting doc ID for the next chunk\n",
    "      print(f\"Processed documents: {start_doc_id - 1}\")\n",
    "#       break  # Exit loop after processing the current chunk\n",
    "\n",
    " \n",
    "\n",
    "# Assuming \"content\" is the column containing text (not necessarily lemmatized)\n",
    "document = document.dropna(subset=[\"content\"])  # Remove rows with missing content\n",
    "\n",
    "# Extract features chunk-wise and save to CSV\n",
    "extract_topic_features(document.copy(), topic_keywords)  # Use copy to avoid modifying original DataFrame\n",
    "\n",
    "print(\"Topic features saved to topic_features_00.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1262e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee8315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
